{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658f39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Merge these imports into the cells where they are used. \n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import typing as t\n",
    "# %pip install pyaqsapi\n",
    "# %pip install certifi\n",
    "# %pip install requests\n",
    "import pyaqsapi as aqs\n",
    "from datetime import date\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4e566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./reference_measurements/aqs_cranston.csv', './reference_measurements/pha.csv', './reference_measurements/pema.csv', './reference_measurements/dpw.csv', './reference_measurements/aqs_myron.csv']\n",
      "['aqs_cranston', 'pha', 'pema', 'dpw', 'aqs_myron']\n"
     ]
    }
   ],
   "source": [
    "# Parse the measurement and reference dataframe lists from csv files in project folder.\n",
    "measurement_files = glob(\"./BEACO2N_measurements/*.csv\")\n",
    "measurement_df = {os.path.splitext(os.path.basename(f))[0] : pd.read_csv(f) for f in measurement_files}\n",
    "reference_files = glob(\"./reference_measurements/*.csv\")\n",
    "reference_df = {os.path.splitext(os.path.basename(f))[0] : pd.read_csv(f) for f in reference_files}\n",
    "print(reference_files)\n",
    "print(list(reference_df.keys()))\n",
    "\n",
    "# Clean measurement and reference data.\n",
    "\n",
    "def clean_BEACO2N(df: pd.DataFrame) -> pd.DataFrame :\n",
    "    # Store time in Pandas datetime format.\n",
    "    df.rename(columns={\"datetime\":\"timestamp\", \"co2_raw\":\"co2\"}, inplace=True)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True).dt.round(\"h\")\n",
    "\n",
    "    # Drop redundant time columns\n",
    "    df.drop(columns=[\"local_timestamp\", \"epoch\", \"node_file_id\", \"node_id\"], inplace=True)\n",
    "    \n",
    "    # For all columns suffixed by \"_wrk_aux\", convert from Volts to milliVolts (*1000) and remove suffix\n",
    "    wrk_aux_cols = df.filter(regex=r\"_wrk_aux$\").columns\n",
    "    df[wrk_aux_cols] *= 1000\n",
    "    df.rename(columns= {col : col.replace(\"_wrk_aux\", \"\") for col in wrk_aux_cols}, inplace=True)\n",
    "\n",
    "    # Use only corrected BEACO2N data.\n",
    "    df.drop(columns='co', inplace=True)\n",
    "    df.rename(columns={'co_corrected':'co'}, inplace=True)\n",
    "\n",
    "    # df['co'] /= 1000 # Convert from ppb to ppm to be consistent with AQS\n",
    "    \n",
    "    # Drop all datapoints with incomplete data (e.g. missing co measurement)\n",
    "    df.dropna(inplace=True)\n",
    "    df.set_index(\"timestamp\")\n",
    "    return df\n",
    "\n",
    "# Clean each site's dataframe\n",
    "measurement_df = {site: clean_BEACO2N(df) for site, df in measurement_df.items()}\n",
    "\n",
    "# Clean data for reference (QuantAQ) analogously\n",
    "# NOTE: QuantAQ CO output is in ppb, because hourly data is 'final' (i.e. corrected). \n",
    "# See: https://docs.quant-aq.com/hardware/modulair/modulair#id-3.1-data-structure-and-outputs\n",
    "def clean_QuantAQ(df: pd.DataFrame) -> pd.DataFrame :\n",
    "    df.drop(columns=[col for col in [\"period_start\", \"period_end\", \"period_end_utc\", \"sn\"] if col in df.columns], inplace=True)\n",
    "    # TODO: Find out if BEACO2N corrected hourly data timestamps reflect the average for the next or previous hour. \n",
    "    df.rename(columns={\"period_start_utc\": \"timestamp\", \"pm25\": \"pm2_5\"}, inplace=True)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
    "    # df['co'] /= 1000 # convert from ppb to ppm [to be consistent with AQS]\n",
    "    df.dropna(inplace=True)\n",
    "    df.set_index(\"timestamp\")\n",
    "    return df\n",
    "\n",
    "reference_df.update({key: clean_QuantAQ(df) for key, df in reference_df.items() if not \"aqs\" in key})\n",
    "\n",
    "def clean_aqs(df: pd.DataFrame) -> pd.DataFrame :\n",
    "    df.rename(columns={\"sample_measurement\" : \"co\"}, inplace=True)\n",
    "    df[\"timestamp\"] = df[\"date_gmt\"] + ' ' + df[\"time_gmt\"]\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True) # should already be hourly, don't need to round.\n",
    "    df.drop(columns=[col for col in df.columns if col not in [\"co\", \"timestamp\"]], inplace=True)\n",
    "    df = df[[\"timestamp\", \"co\"]]\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df.set_index(\"timestamp\")\n",
    "    return df\n",
    "\n",
    "reference_df.update({key : clean_aqs(df) for key, df in reference_df.items() if \"aqs\" in key})\n",
    "\n",
    "# Remove AQS references from keys\n",
    "# TODO: Temporary\n",
    "# reference_df = {key : val for key, val in reference_df.items() if \"aqs\" not in key}\n",
    "\n",
    "# Remove CO outliers from every site's data, including reference. \n",
    "def rm_reference_outliers(df : pd.DataFrame) -> pd.DataFrame :\n",
    "    co_zscore = abs(df['co']-df['co'].mean())/df['co'].std()\n",
    "    return df[co_zscore < 3]\n",
    "def rm_measurement_outliers(df : pd.DataFrame) -> pd.DataFrame :\n",
    "    co_zscore = abs(df['co']-df['co'].mean())/df['co'].std()\n",
    "    return df[co_zscore < 3]\n",
    "\n",
    "measurement_df = {site: rm_measurement_outliers(df) for site, df in measurement_df.items()}\n",
    "reference_df = {key: rm_reference_outliers(df) for key, df in reference_df.items()}\n",
    "\n",
    "zones = {\n",
    "    \"dpw\" : [\"reservoir\", \"medschool\", \"dpw\", \"ccri\", \"southprovlib\", \"prek\", \"gym\", \"cfs\", \"myron\"],\n",
    "    \"pema\" : [\"ecubed\", \"rochambeau\", \"smithhill\", \"martialarts\", \"blackstone\", \"rocklib\", \"provcollege\", \"pema\"],\n",
    "    \"pha\" : [\"silverlake\", \"carnevale\", \"zuccolo\", \"wecc\", \"unitedway\", \"pha\", \"mtpleasant\", \"ricollege\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12347584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Prepare a list to collect all dataframes with renamed columns\n",
    "dfs = []\n",
    "\n",
    "# Helper function to rename columns for each site\n",
    "def rename_columns(df, site, is_aqs=False, is_quantaq=False, is_beaco2n=False):\n",
    "    # Only keep timestamp, co, rh, temp if present\n",
    "    cols = ['timestamp']\n",
    "    if 'co' in df.columns: cols.append('co')\n",
    "    if 'rh' in df.columns: cols.append('rh')\n",
    "    if 'temp' in df.columns: cols.append('temp')\n",
    "    df = df[cols].copy()\n",
    "    if is_aqs:\n",
    "        # For AQS, use \"co_aqs_[site]\" (not \"co_aqs_[site]_aqs\")\n",
    "        suffix = f\"_aqs_{site}\"\n",
    "    elif is_quantaq:\n",
    "        suffix = f\"_{site}_quantaq\"\n",
    "    elif is_beaco2n:\n",
    "        suffix = f\"_{site}_beaco2n\"\n",
    "    else:\n",
    "        suffix = f\"_{site}\"\n",
    "    rename_map = {}\n",
    "    if 'co' in df.columns:\n",
    "        if is_aqs:\n",
    "            rename_map['co'] = f\"co_{site}\"\n",
    "        else:\n",
    "            rename_map['co'] = f\"co{suffix}\"\n",
    "    if 'rh' in df.columns: rename_map['rh'] = f\"rh{suffix}\"\n",
    "    if 'temp' in df.columns: rename_map['temp'] = f\"temp{suffix}\"\n",
    "    df = df.rename(columns=rename_map)\n",
    "    return df\n",
    "\n",
    "# Add reference sites\n",
    "for site, df in reference_df.items():\n",
    "    is_aqs = 'aqs' in site\n",
    "    is_quantaq = (site in measurement_df) and not is_aqs\n",
    "    dfs.append(rename_columns(df, site, is_aqs=is_aqs, is_quantaq=is_quantaq))\n",
    "\n",
    "# Add measurement sites\n",
    "for site, df in measurement_df.items():\n",
    "    is_beaco2n = (site in reference_df)\n",
    "    dfs.append(rename_columns(df, site, is_beaco2n=is_beaco2n))\n",
    "\n",
    "# Outer merge all dataframes on timestamp\n",
    "merged_all = reduce(lambda left, right: pd.merge(left, right, on='timestamp', how='outer'), dfs)\n",
    "merged_all = merged_all.sort_values('timestamp').set_index('timestamp')\n",
    "\n",
    "merged_all.index.rename(\"date\", inplace=True)\n",
    "merged_all.to_csv(\"./combined_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
