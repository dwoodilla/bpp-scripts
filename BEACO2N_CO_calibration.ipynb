{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a35cba-cd58-4f6c-ad54-2007cf300c04",
   "metadata": {},
   "source": [
    "This script calibrates BEACO2N carbon monoxide data using QuantAQ sensors as reference. The script only applies this calibration to colocated sites, i.e. Department of Public Works, Providence Emergency Management Agency, and Providence Housing Authority. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbdeb7-d592-46dc-814b-2a8dbe61ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the measurement and reference dataframe lists from csv files in project folder.\n",
    "measurement_files = glob(\"./BEACO2N_measurements/*.csv\")\n",
    "measurement = {os.path.splitext(os.path.basename(f))[0] : pd.read_csv(f) for f in measurement_files}\n",
    "reference_files = glob(\"./reference_measurements/*.csv\")\n",
    "reference = {os.path.splitext(os.path.basename(f))[0] : pd.read_csv(f) for f in reference_files}\n",
    "\n",
    "# Clean measurement and reference data.\n",
    "\n",
    "def clean_measurement(df: pd.DataFrame) -> pd.DataFrame :\n",
    "    # Store time in Pandas datetime format.\n",
    "    df = df.rename(columns={\"datetime\":\"timestamp\", \"co2_raw\":\"co2\"})\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True).dt.round(\"h\")\n",
    "\n",
    "    # Drop redundant time columns\n",
    "    df = df.drop(columns=[col for col in [\"local_timestamp\", \"epoch\", \"node_file_id\", \"node_id\"] if col in df.columns])\n",
    "    \n",
    "    # For all columns suffixed by \"_wrk_aux\", convert from Volts to milliVolts (*1000) and remove suffix\n",
    "    wrk_aux_cols = df.filter(regex=r\"_wrk_aux$\").columns\n",
    "    df[wrk_aux_cols] *= 1000\n",
    "    df.rename(columns= {col : col.replace(\"_wrk_aux\", \"\") for col in wrk_aux_cols}, inplace=True)\n",
    "    \n",
    "    # Drop all datapoints with incomplete data (e.g. missing co measurement)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# Clean each site's dataframe\n",
    "measurement = {site: clean_measurement(df) for site, df in measurement.items()}\n",
    "\n",
    "# Clean data for reference (QuantAQ) analogously\n",
    "def clean_reference(df: pd.DataFrame) -> pd.DataFrame :\n",
    "    df = df.drop(columns=[col for col in [\"period_start\", \"period_end\", \"period_end_utc\", \"sn\"] if col in df.columns])\n",
    "    df = df.rename(columns={\"period_start_utc\": \"timestamp\", \"pm25\": \"pm2_5\"})\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "reference = {key: clean_reference(df) for key, df in reference.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99064e9",
   "metadata": {},
   "source": [
    "Find time intervals when RSD(co) < .10 for all reference sensors in the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d9fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "rsd_df = []\n",
    "\n",
    "# Add each site's timestamp and co data to a list of co tables indexed by site\n",
    "[rsd_df.append(df[[\"timestamp\",\"co\"]].rename(columns={'co':site})) for site, df in reference.items()]\n",
    "# Merge the co tables into one table. \n",
    "rsd_df = reduce(lambda table, to_merge: pd.merge(table, to_merge, on=\"timestamp\", how=\"inner\"), rsd_df) # type: ignore\n",
    "\n",
    "def rsd(row:pd.Series) -> float:\n",
    "    ''' Helper function to calculate residual standard deviation of a dataframe row. '''\n",
    "    vals = row[1:].values.astype(float)\n",
    "    mean = np.mean(vals)\n",
    "    sd = np.std(vals)\n",
    "    return float(sd/mean) if mean != 0 else np.nan\n",
    "\n",
    "rsd_df[\"rsd\"]=rsd_df.apply(rsd, axis=1)\n",
    "timestamps_rsd_lt_10pc = rsd_df[rsd_df[\"rsd\"]<.10][\"timestamp\"]\n",
    "\n",
    "# Filter datasets to include only data contained by intersection(timestamps_rsd_lt_10pc, measurement, reference)\n",
    "\n",
    "# Find the common timestamps\n",
    "common_timestamps = timestamps_rsd_lt_10pc\n",
    "for site in measurement:\n",
    "    common_timestamps = common_timestamps[common_timestamps.isin(measurement[site][\"timestamp\"])]\n",
    "for site in reference:\n",
    "    common_timestamps = common_timestamps[common_timestamps.isin(reference[site][\"timestamp\"])]\n",
    "\n",
    "# Filter dataframes to only include timestamps in common_timestamps\n",
    "for site in measurement.keys():\n",
    "    measurement[site] = measurement[site][measurement[site][\"timestamp\"].isin(common_timestamps)].reset_index(drop=True).sort_values(\"timestamp\")\n",
    "for site in reference.keys():\n",
    "    reference[site] = reference[site][reference[site][\"timestamp\"].isin(common_timestamps)].reset_index(drop=True).sort_values(\"timestamp\")\n",
    "\n",
    "# # Print the first few rows of each key dataframe for inspection\n",
    "# print(\"rsd_df (first 5 rows):\")\n",
    "# display(rsd_df.head())\n",
    "\n",
    "# for name, data in [(\"merged\", merged), (\"measurement\", measurement), (\"reference\", reference)]:\n",
    "#     print(f\"\\n{name} (first 2 rows per site):\")\n",
    "#     for k, v in data.items():\n",
    "#         print(f\"Site: {k}\")\n",
    "#         display(v.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490f84e",
   "metadata": {},
   "source": [
    "Fit per-site regression models to the data where RSC(co)<0.10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f48ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_sites = list(reference.keys())\n",
    "zones = {\n",
    "    \"dpw\" : [\"reservoir\", \"medschool\", \"dpw\", \"ccri\", \"southprovlib\", \"prek\", \"gym\", \"cfs\", \"myron\"],\n",
    "    \"pema\" : [\"ecubed\", \"rochambeau\", \"smithhill\", \"martialarts\", \"blackstone\", \"rocklib\", \"provcollege\", \"pema\"],\n",
    "    \"pha\" : [\"silverlake\", \"carnevale\", \"zuccolo\", \"wecc\", \"unitedway\", \"pha\", \"mtpleasant\", \"ricollege\"]\n",
    "}\n",
    "\n",
    "models = {ref_site : {meas_site : LinearRegression() for meas_site in zones[ref_site]} for ref_site in zones.keys()}\n",
    "X_train = {ref_site : {meas_site : pd.DataFrame() for meas_site in zones[ref_site]} for ref_site in zones.keys()}\n",
    "X_test = {ref_site : {meas_site : pd.DataFrame() for meas_site in zones[ref_site]} for ref_site in zones.keys()}\n",
    "y_train = {ref_site : pd.DataFrame() for ref_site in zones.keys()}\n",
    "y_test = {ref_site : pd.DataFrame() for ref_site in zones.keys()}\n",
    "y_pred = {ref_site : pd.DataFrame() for ref_site in zones.keys()} #TODO: This is double-indexed below and is thus of wrong type\n",
    "\n",
    "for i, ref_site in enumerate(zones.keys()):\n",
    "    train_indxs, test_indxs = train_test_split(range(len(reference[ref_site])), random_state=0)\n",
    "    y_train[ref_site] = reference[ref_site].iloc[train_indxs]\n",
    "    y_test[ref_site] = reference[ref_site].iloc[test_indxs]\n",
    "\n",
    "    for j, meas_site in enumerate(zones.get(ref_site)): # type: ignore\n",
    "        X_train[ref_site][meas_site] = measurement[meas_site].iloc[train_indxs].drop(\"timestamp\", axis=1)\n",
    "        X_test[ref_site][meas_site] = measurement[meas_site].iloc[train_indxs].drop(\"timestamp\", axis=1)\n",
    "        models[ref_site][meas_site].fit(X_train[ref_site][meas_site], y_train[ref_site])\n",
    "        # TODO: This double-indexes y_pred above which will cause bug.\n",
    "        y_pred[ref_site][meas_site] = models[ref_site][meas_site].predict(X_test[ref_site][meas_site])\n",
    "# for i, site in enumerate(ref_sites):\n",
    "#     x_train_i, x_test_i, y_train_i, y_test_i = train_test_split(measurement[site].drop(\"timestamp\", axis=1), reference[site][\"co\"], random_state=42)\n",
    "#     x_train[site] = x_train_i\n",
    "#     x_test[site] = x_test_i\n",
    "#     y_train[site] = y_train_i\n",
    "#     y_test[site] = y_test_i\n",
    "#     models[site] = LinearRegression()\n",
    "#     models[site].fit(x_train[site], y_train[site])\n",
    "#     y_pred[site] = models[site].predict(x_test[site])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70a611",
   "metadata": {},
   "source": [
    "Display residual graphs and statistics for each site model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c1550",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=3, subplot_titles=[f\"{site}: R^2={models[site].score(x_test[site], y_test[site]):.4f}\" for site in refsites])\n",
    "fig_ledgend = {\"Corrected CO\":\"blue\", \"Uncorrected CO\":\"red\"}\n",
    "hover_vals = {\"Corrected CO\":\n",
    "            'Reference CO: %{x}<br>' + \n",
    "            'Corrected CO:%{y}<br>' +\n",
    "            'Timestamp: %{customdata}<extra></extra>', \n",
    "            \"Uncorrected CO\":\n",
    "            'Reference CO: %{x}<br>' + \n",
    "            'Uncorrected CO:%{y}<br>' +\n",
    "            'Timestamp: %{customdata}<extra></extra>'}\n",
    "for i, site in enumerate(refsites):\n",
    "    # Plot each site's data in a subplot\n",
    "    showledgend = (i==1)\n",
    "    y_uncorrected = measurement[site].sort_values(by=\"timestamp\")[\"co\"]\n",
    "    for label in fig_ledgend.keys():\n",
    "        # Plot corrected and uncorrected CO data for each site\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=y_test[site],\n",
    "            y = y_pred[site] if label==\"Corrected CO\" else y_uncorrected,\n",
    "            mode='markers',\n",
    "            name=label,\n",
    "            marker_color=fig_ledgend[label],\n",
    "            showlegend=showledgend,\n",
    "            customdata=common_timestamps.sort_values(),\n",
    "            hovertemplate=hover_vals[label] \n",
    "        ), \n",
    "        row=1, col=(i+1))\n",
    "\n",
    "    min_val = min(y_test[site].min(), y_pred[site].min(), y_uncorrected.min())\n",
    "    max_val = max(y_test[site].max(), y_pred[site].max(), y_uncorrected.max())\n",
    "    fig.add_shape(\n",
    "        type='line',\n",
    "        x0=min_val, y0=min_val,\n",
    "        x1=max_val, y1=max_val,\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        name='1:1 Line',\n",
    "        row=1, col=(i+1)\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=\"Corrected BEACO2N CO data at reference sites:\",\n",
    "        xaxis_title=\"Reference CO (mV)\",\n",
    "        yaxis_title=\"Measured CO (mV)\",\n",
    "        legend_title=\"Legend\", \n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Reference CO (mV)\")\n",
    "\n",
    "fig.show()\n",
    "# Collect coefficients for each site into a DataFrame for tabular display\n",
    "coef_table = []\n",
    "for site, model in models.items():\n",
    "    for name, coef in zip(x_train[site].columns, model.coef_):\n",
    "        coef_table.append({'Site': site, 'Parameter': name, 'Coefficient': coef})\n",
    "coef_df = pd.DataFrame(coef_table)\n",
    "display(coef_df.pivot(index='Parameter', columns='Site', values='Coefficient').round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb4cd5",
   "metadata": {},
   "source": [
    "Assign each measurement (BEACO2N) sensor to a reference sensor and train a calibration model (for each measurement sensor) using timestamps with RSC(co)<0.10 within the reference network.\n",
    "\n",
    "Note: Measurement nodes are assigned to the nearest reference node according to calculations done in QGIS with Grace's BPP network map. May be worth confirming this using ArcGIS at some point. (Perhaps RIDEM data can eventually be used to improve spacial accuracy... some nodes are >2mi from reference.)\n",
    "\n",
    "| Reference | Measurement locations |\n",
    "|------------|-------------------------|\n",
    "| dpw | reservoir, medschool, dpw, ccri, southprovlib, prek, gym, cfs, myron|\n",
    "| pema | ecubed, rochambeau, smithhill, martialarts, blackstone, rocklib, provcollege, pema|\n",
    "| pha | silverlake, carnevale, zuccolo, wecc, unitedway, pha, mtpleasant, ricollege|\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
